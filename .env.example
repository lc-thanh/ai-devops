# AI DevOps Assistant - Environment Configuration
# Copy this file to .env and configure as needed

# =============================================================================
# Server Configuration
# =============================================================================

# HTTP port to listen on
PORT=8080

# Server read timeout (duration or seconds)
SERVER_READ_TIMEOUT=30s

# Server write timeout (duration or seconds)
SERVER_WRITE_TIMEOUT=30s

# Gin mode: debug, release, test
GIN_MODE=debug

# =============================================================================
# AI Configuration
# =============================================================================

# AI provider: openai or gemini
AI_PROVIDER=openai

# API key for the AI provider (required unless AI_MOCK_MODE=true)
AI_API_KEY=your_api_key_here

# Base URL for AI API (provider-specific defaults apply)
# For OpenAI: https://api.openai.com/v1
# For Azure OpenAI: https://YOUR_RESOURCE.openai.azure.com/openai/deployments/YOUR_DEPLOYMENT
# For Gemini: https://generativelanguage.googleapis.com
AI_BASE_URL=https://api.openai.com/v1

# AI model to use
# OpenAI models: gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-3.5-turbo
# Gemini models: gemini-2.0-flash, gemini-1.5-flash, gemini-1.5-pro, gemini-1.0-pro
AI_MODEL=gpt-4o-mini

# Maximum time to wait for AI response (duration or seconds)
AI_TIMEOUT=30s

# Maximum tokens for AI response
AI_MAX_TOKENS=1024

# Number of retries on transient failures
AI_MAX_RETRIES=2

# Enable mock mode for testing without API calls
# Set to true for CI/CD or development without API access
AI_MOCK_MODE=false

# =============================================================================
# Gemini-specific Configuration Example
# =============================================================================
# To use Gemini, set:
#   AI_PROVIDER=gemini
#   AI_API_KEY=your_gemini_api_key
#   AI_MODEL=gemini-2.0-flash
# The base URL will default to https://generativelanguage.googleapis.com

# =============================================================================
# Processing Configuration
# =============================================================================

# Maximum log size in bytes (logs larger than this will be truncated)
MAX_LOG_SIZE=50000

# Enable rule-based pre-classification
# When true, known patterns are handled without AI for faster response
ENABLE_RULES=true

# Minimum confidence threshold to use rule-based results (0.0-1.0)
# Higher values mean stricter matching
RULE_CONFIDENCE_THRESHOLD=0.8

# =============================================================================
# Logging Configuration
# =============================================================================

# Log level: debug, info, warn, error
LOG_LEVEL=info
